# `miniformer`

This repository contains the implementation and resources for GPT-2, a transformer-based language model.

## Structure

- `miniformer/` - Source code for model architecture and training scripts.
- `data/` - Datasets and preprocessing scripts.
- `notebooks/` - Jupyter notebooks for experiments and analysis.
- `requirements.txt` - Python dependencies.

## Getting Started

1. Clone the repository.
2. Install dependencies:  
    ```bash
    pip install -r requirements.txt
    ```
3. Explore the notebooks or run training scripts in `src/`.

## License

See `LICENSE` for details.